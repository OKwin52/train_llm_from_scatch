# Readme

This project is about training a large language model from scratch for personal learning.

I will record some relevant learning experiences here.

Perhaps it will develop into a project summary of the learning route for individuals to learn large language models in the future...

Get the tokenizer, SFT and DPO training datasets from this link: [https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files%EF%BC%8C%E9%9A%8F%E5%90%8E%E8%BF%90%E8%A1%8C)

Then run `train_tokenizer.py`to train the tokenizer.